import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt 
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import RandomOverSampler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import precision_score, recall_score
from sklearn.neighbors import KNeighborsClassifier


# Read the dataset
df = pd.read_csv("Data\\creditcard.csv")  # Source: Kaggle 'https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud'

# Splitting the data into test, train and validation sets
train, valid, test = np.split(
                    df.sample(frac=1),
                    [int(0.6 * len(df)), int(0.8 * len(df))]
)

def sanityCheck(tra, tes, val):
    '''Print sample heads and sizes of the data splits'''

    print('\n----  TRAINING DATA  ----')
    print(tra.head())
    print('\n----  TESTING DATA  ----')
    print(tes.head())
    print('\n----  VALIDATION DATA  ----')
    print(val.head())
    
    print('\nTraining Data Length')
    print(len(tra))
    print('\nTesting Data Length')
    print(len(tes))
    print('\nValidation Data Length')
    print(len(val))



# Scales data frame
def scaledData(dataframe, oversample = False):
    x = dataframe[dataframe.columns[:-1]].values
    y = dataframe[dataframe.columns[-1]].values

    x = StandardScaler().fit_transform(x)

    if oversample:
        ros = RandomOverSampler(random_state=67)
        x, y = ros.fit_resample(x, y)

    data = np.hstack((x, np.reshape(y, (-1, 1))))

    return data, x, y

train, x_train, y_train = scaledData(train, True)
test, x_test, y_test = scaledData(test)
valid, x_valid, y_valid = scaledData(valid)


# Logistic Regression Model, Liz
liz = LogisticRegression()
liz.fit(x_train, y_train)

testP_liz = liz.predict(x_test)

print('LOGISTIC REGRESSION')
print(recall_score(y_test, testP_liz))
print(precision_score(y_test, testP_liz))

# KNN Model, Kenny
# kenny = KNeighborsClassifier(n_neighbors=3)  # N = 3
# kenny.fit(x_train, y_train)

# testP_ken = kenny.predict(x_test)

# print('KNN')
# print(recall_score(y_test, testP_ken))
# print(precision_score(y_test, testP_ken))


# FOR KNN FIRST TRY N=3 and N=5 
# THEN TRY CROSS VALIDATION TO FIND A BETTER K

# TRY SVM


# sanityCheck(train, test, valid)